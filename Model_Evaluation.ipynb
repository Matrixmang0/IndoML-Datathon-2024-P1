{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9kZWkMw9WK3"
      },
      "source": [
        "## Evaluating Model Performance by Category\n",
        "\n",
        "In this cell, we evaluate the model's performance by splitting the generated and target details into categories and calculating various metrics:\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "- **`generated_dict`** and **`target_dict`**: Dictionaries to store generated and target details for each category (0 through 5). The `generated_details` and `target_details` lists are split into these dictionaries based on category indices.\n",
        "\n",
        "- **`generated_details`** and **`target_details`**: Predictions generated by the model and the actual target values\n",
        "\n",
        "### Metrics Calculation\n",
        "\n",
        "- **`categories`**: List of categories for which metrics will be computed: `details_Brand`, `L0_category`, `L1_category`, `L2_category`, `L3_category`, and `L4_category`.\n",
        "\n",
        "- **`metrics`**: List of metrics to be calculated: `accuracy`, `precision`, `recall`, and `f1`.\n",
        "\n",
        "For each category:\n",
        "1. **Compute Metrics**: Accuracy, precision, recall, and F1 score are calculated using `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics`. Metrics are computed with macro averaging to handle multi-class classification.\n",
        "\n",
        "2. **Print Results**: The results for each category are printed, showing the calculated metrics with four decimal places.\n",
        "\n",
        "The printed results provide insight into the performance of the model across different categories and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qrwLQT69WK3",
        "outputId": "72511060-cce6-4504-c7a8-861f4ac357f6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "generated_dict = {i: [] for i in range(6)}\n",
        "target_dict = {i: [] for i in range(6)}\n",
        "\n",
        "for gen, tar in zip(generated_details, target_details):\n",
        "    for i in range(6):\n",
        "        generated_dict[i].append(gen[i])\n",
        "        target_dict[i].append(tar[i])\n",
        "\n",
        "print('Splitted into category.............\\n')\n",
        "\n",
        "# Clean repeated patterns in L4_category\n",
        "generated_dict[5] = [text for text in generated_dict[5]]\n",
        "\n",
        "categories = ['details_Brand', 'L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "results = {category: {metric: 0 for metric in metrics} for category in categories}\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    print('Current Category: ', category)\n",
        "    y_true = target_dict[i]\n",
        "    y_pred = generated_dict[i]\n",
        "\n",
        "    results[category]['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    results[category]['precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    results[category]['recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    results[category]['f1'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print()\n",
        "\n",
        "for category, metrics in results.items():\n",
        "    print(f\"{category}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T69-lgL99WK3"
      },
      "source": [
        "## Computing Item-Level Accuracy\n",
        "\n",
        "In this cell, we define a function to compute item-level accuracy, which measures how often all predicted categories match the target categories for each item:\n",
        "\n",
        "### Function: `compute_item_accuracy`\n",
        "\n",
        "- **Inputs**:\n",
        "  - `generated_details`: List of predicted details for each item.\n",
        "  - `target_details`: List of true details for each item.\n",
        "\n",
        "- **Process**:\n",
        "  - **Count Correct Items**: Iterates through pairs of generated and target details. If all elements in a generated detail match the corresponding elements in the target detail, it counts as a correct item.\n",
        "  - **Compute Accuracy**: Divides the count of correct items by the total number of items to get the accuracy. Returns `0` if there are no items.\n",
        "\n",
        "### Execution\n",
        "\n",
        "- **`item_accuracy`**: Calls `compute_item_accuracy` with the `generated_details` and `target_details` to calculate the accuracy.\n",
        "- **Print Accuracy**: Prints the item-level accuracy with four decimal places.\n",
        "\n",
        "Item-level accuracy provides a metric of how well the model performs in predicting all categories correctly for each product.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PChNHPN9WK4",
        "outputId": "79b20afd-4ebc-48ee-aff9-e8c4479ecd33"
      },
      "outputs": [],
      "source": [
        "def compute_item_accuracy(generated_details, target_details):\n",
        "    correct_items = 0\n",
        "    total_items = len(generated_details)\n",
        "\n",
        "    for gen, tar in zip(generated_details, target_details):\n",
        "        if all(g == t for g, t in zip(gen, tar)):\n",
        "            correct_items += 1\n",
        "\n",
        "    return correct_items / total_items if total_items > 0 else 0\n",
        "\n",
        "item_accuracy = compute_item_accuracy(generated_details, target_details)\n",
        "print(f\"Item-level accuracy: {item_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
